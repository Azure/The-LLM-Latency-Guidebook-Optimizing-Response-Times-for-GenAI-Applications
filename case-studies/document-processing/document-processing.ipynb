{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Document Processing\n",
    "\n",
    "This case study covers an application which is used to correct a document for spelling errors and grammar mistakes.\n",
    "\n",
    "More advanced applications might be used to implement the specific style guides of an organization, or review documents for statements that create exposure to legal risk, and so on.\n",
    "\n",
    "LLMs are a powerful tool for document processing, however as the documents get longer, the time taken for the application to complete the task becomes increasingly important. If the application is an overnight batch process, this is less of a concern, however if a user is waiting for the output to continue their workflow, then the response time is key to the user experience.\n",
    "\n",
    "The techniques shown here show an approximate 100x speed up in the processing of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import copy\n",
    "import textwrap\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def aoai_call(system_message,prompt,model):\n",
    "    client = AzureOpenAI(\n",
    "        api_version=os.getenv(\"API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"API_KEY\")\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    e2e_time = end_time - start_time\n",
    "\n",
    "    result=json.loads(completion.model_dump_json(indent=2))\n",
    "    prompt_tokens=result[\"usage\"][\"prompt_tokens\"]\n",
    "    completion_tokens=result[\"usage\"][\"completion_tokens\"]\n",
    "    completion_text=result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return result,prompt_tokens,completion_tokens,completion_text,e2e_time\n",
    "\n",
    "model=os.getenv(\"MODELGPT432k\")\n",
    "\n",
    "# Read essay from a text file\n",
    "with open('document_with_errors.txt', 'r') as f:\n",
    "    document_with_errors = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Case: LLM used to rewrite document and implement corrections\n",
    "\n",
    "**Time taken: 315 seconds**\n",
    "\n",
    "In this example, the LLM is rewriting the document and correcting the errors, which is included in the JSON key \"rewritten_document\". The process is chunked, as at times the LLM will refuse to rewrite long documents. By chunking it, the LLM will write out the document with all the corrections.\n",
    "\n",
    "The JSON also includes other parameters, where for each error in the document there is an explanation of why it was an error, the incorrect text, and the corrected text, so that in the front end of an application, the corrections are presented to a human, who checks the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prompt Tokens: 3292\n",
      "Total Completion Tokens: 3725\n",
      "Total Cost: $0.6445\n",
      "Total Time Taken: 315.50 seconds\n",
      "Rewritten Document: \n",
      "List of Errors to Correct: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Initialize variables\n",
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_time = 0\n",
    "rewritten_document = \"\"\n",
    "list_of_errors_to_correct = []\n",
    "completion_text_list=[]\n",
    "\n",
    "# Split the text into chunks of words\n",
    "chunks = textwrap.wrap(document_with_errors, 1000)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    system_message=\"\"\"\n",
    "    You are a helpful AI assistant.\n",
    "    \"\"\"\n",
    "    prompt=f\"\"\"\n",
    "    Document to correct:\n",
    "    {chunk}\n",
    "\n",
    "    \"\"\"\n",
    "    prompt=prompt+\"\"\"\n",
    "    The output should be a JSON object. Only return the JSON object, with no comments or additional text.\n",
    "    Use this structure:\n",
    "    {\n",
    "        \"rewritten_document\": \"The document with the errors corrected.\",\n",
    "        \"list_of_errors_to_correct\": [\n",
    "            {\n",
    "                \"explanation_of_error\": \"Think step-by-step about identifying potential spelling errors or grammar issues. Consider all errors together and consider the whole sentence of text before applying a rule.\",\n",
    "                \"incorrect_text\": \"If there is a error fill this with the errors text sub-string, consider the full sentence and context of the incorrect text before filling this in\",\n",
    "                \"fixed_text\": \"Think step by step about the error and then fill this with the fixed version of the text, don't apply any other fixes apart from the error if there is one\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "    total_prompt_tokens += prompt_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    total_time += e2e_time\n",
    "    completion_text_list.append(completion_text)\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming completion_text_list is your list of dictionaries\n",
    "combined_list = {\"list_of_errors_to_correct\":[],\"rewritten_document\":\"\"}\n",
    "\n",
    "for completion_text in completion_text_list:\n",
    "    json_string = completion_text.replace('\\n', '')\n",
    "    data = json.loads(json_string)\n",
    "    if not data[\"list_of_errors_to_correct\"]:\n",
    "        pass\n",
    "    else:\n",
    "        combined_list[\"list_of_errors_to_correct\"].append(data[\"list_of_errors_to_correct\"])\n",
    "        combined_list[\"rewritten_document\"]=combined_list[\"rewritten_document\"]+data[\"rewritten_document\"]\n",
    "\n",
    "document_with_corrections=combined_list[\"rewritten_document\"]\n",
    "# print(document_with_corrections)\n",
    "\n",
    "\n",
    "# Print totals\n",
    "# print(f\"Total Prompt Tokens: {total_prompt_tokens}\")\n",
    "# print(f\"Total Completion Tokens: {total_completion_tokens}\")\n",
    "print(f\"Total Time Taken: {total_time:.2f} seconds\")\n",
    "# print(f\"List of Errors to Correct: {list_of_errors_to_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the \"avoid-rewriting-documents\" technique\n",
    "\n",
    "**Time taken: 38 seconds**\n",
    "\n",
    "Rewriting the entire document is very slow, as the length of the response will be proportionate to the length of the document. It will also be proportionate to the number of errors, as it will need to explain each mistake.\n",
    "\n",
    "Instead of rewriting the entire document, the LLM is used to identify the mistakes, and python code is used for correcting the text, by replacing only the sections that are wrong (similar to how a human would do it!). \n",
    "\n",
    "The speed of the application is now proportionate to the number of mistakes in the document- this of course will vary from document to document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prompt Tokens: 3255\n",
      "Total Completion Tokens: 421\n",
      "Total Time Taken: 38.15 seconds\n",
      "Rewritten Document: \n",
      "List of Errors to Correct: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Initialize variables\n",
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_time = 0\n",
    "rewritten_document = \"\"\n",
    "list_of_errors_to_correct = []\n",
    "completion_text_list=[]\n",
    "\n",
    "# Split the text into chunks of words\n",
    "chunks = textwrap.wrap(document_with_errors, 1000)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    system_message=\"\"\"\n",
    "    You are a helpful AI assistant.\n",
    "    \"\"\"\n",
    "    prompt=f\"\"\"\n",
    "    Document to correct:\n",
    "    {chunk}\n",
    "\n",
    "    \"\"\"\n",
    "    prompt=prompt+\"\"\"\n",
    "    The output should be a JSON object. Only return the JSON object, with no comments or additional text.\n",
    "    Use this structure:\n",
    "    {\n",
    "        \"list_of_errors_to_correct\": [\n",
    "            {\n",
    "                \"explanation_of_error\": \"Think step-by-step about identifying potential spelling errors or grammar issues. Consider all errors together and consider the whole sentence of text before applying a rule.\",\n",
    "                \"incorrect_text\": \"If there is a error fill this with the errors text sub-string, consider the full sentence and context of the incorrect text before filling this in\",\n",
    "                \"fixed_text\": \"Think step by step about the error and then fill this with the fixed version of the text, don't apply any other fixes apart from the error if there is one\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "    total_prompt_tokens += prompt_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    total_time += e2e_time\n",
    "    completion_text_list.append(completion_text)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming completion_text_list is your list of dictionaries\n",
    "combined_list = {\"list_of_errors_to_correct\":[]}\n",
    "\n",
    "for completion_text in completion_text_list:\n",
    "    json_string = completion_text.replace('\\n', '')\n",
    "    data = json.loads(json_string)\n",
    "    if not data[\"list_of_errors_to_correct\"]:\n",
    "        pass\n",
    "    else:\n",
    "        combined_list[\"list_of_errors_to_correct\"].append(data[\"list_of_errors_to_correct\"])\n",
    "\n",
    "# Assuming document_with_errors is your text document\n",
    "for error_list in combined_list[\"list_of_errors_to_correct\"]:\n",
    "    for error_dict in error_list:\n",
    "        document_with_errors = document_with_errors.replace(error_dict['violating_text'], error_dict['fixed_text'])\n",
    "document_with_corrections=document_with_errors\n",
    "# print(document_with_corrections)\n",
    "\n",
    "## Print totals\n",
    "# print(f\"Total Prompt Tokens: {total_prompt_tokens}\")\n",
    "# print(f\"Total Completion Tokens: {total_completion_tokens}\")\n",
    "print(f\"Total Time Taken: {total_time:.2f} seconds\")\n",
    "# print(f\"List of Errors to Correct: {list_of_errors_to_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the \"prompt-for-concision\" technique\n",
    "\n",
    "**Time taken: 20 seconds**\n",
    "\n",
    "Prompting the model to be concise when explaining the errors may lead further improvements. In real world testing, this added up to a 2x improvement.\n",
    "\n",
    "This is because the explanation of the error, unless it is particularly complex, can be truncated from the default model's verbosity to 2-3 words. It is also possible to save additional time, by asking the model for only the shortest amount of text possible for replacement.\n",
    "\n",
    "Lastly, the JSON structure has been changed to an array. By avoiding writing the JSON keys for every single mistake, even further token generation has been prevented, speeding up the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prompt Tokens: 6006\n",
      "Total Completion Tokens: 96\n",
      "Total Time Taken: 24.35 seconds\n",
      "Rewritten Document: \n",
      "List of Errors to Correct: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Initialize variables\n",
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_time = 0\n",
    "rewritten_document = \"\"\n",
    "list_of_errors_to_correct = []\n",
    "completion_text_list=[]\n",
    "\n",
    "# Split the text into chunks of words\n",
    "chunks = textwrap.wrap(document_with_errors, 1000)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    system_message=\"\"\"\n",
    "    You are a helpful AI assistant.\n",
    "    \"\"\"\n",
    "    prompt=\"\"\"\n",
    "    The output should be a list object. Only return the list object, with no comments or additional text.\n",
    "    Use this structure:\n",
    "    [[\"The first item in the list is an explanation of the error. Think step-by-step about identifying potential spelling errors or grammar issues. Consider all errors together and consider the whole sentence of text before applying a rule. Explain the error in the shortest sentence possible, ideally 3 to 7 words.\",\"The second item in this list is the specific text that includes the error, typically around 3 words either side of the error. If there is a error fill this with the errors text sub-string, consider the full sentence and context of the incorrect text before filling this in. When selecting the substring of the text, use the shortest amount of text whilst ensuring the sub string is unique in the document.\",\"The third item is the corrected text. It should be exactly the same string as the second item, but with the spelling error or grammar corrected. Think step by step about the error and then fill this with the fixed version of the text.\"]]\n",
    "\n",
    "    START_EXAMPLE_1\n",
    "    \n",
    "    INPUT_DOCUMENT:\n",
    "    Start Your Day Positively: Begin your mornings with small victories. Accomplish a tinny task, like making your bed or enjoying a cup of coffee. Set an intention for the day, whether it’s a guiding principle or a specific action you’ll take. Delayed using your phone upon waking up and replace social media scrolling with gratitude. Reflect on simple things you’re thankful for, like someone holding the door open or a warm cup of coffee from your partner.\n",
    "    Prioritize Self-Care Throughout the Day: As the day progresses, continue nurturing your well-being. Savor encouraging words, read inspirational quotes, or revisit kind texts from friends. Stay active by taking short walks or practicing mindfulness. Remember that small, consistent efforts add up, contributing to your overall health and happiness. \n",
    "    OUTPUT_LIST:[[\"Verb tense\",\"Delayed using your\",\"Delay using your\"],[\"Spelling\",\"a tinny task\",\"a tiny task\"]]\n",
    "\n",
    "    Do not output \\n and many whitespaces.\n",
    "\n",
    "    If there are no errors, return a list like this:[[\"NA\",\"NA\",\"NA\"]]. Do not generate output like this:['\\n    [[\"NA\",\"NA\",\"NA\"]]']\n",
    "    \"\"\"\n",
    "\n",
    "    prompt=prompt+f\"\"\"\n",
    "    INPUT_DOCUMENT:\n",
    "    {chunk}\n",
    "    OUTPUT_LIST:\n",
    "    \"\"\"\n",
    "\n",
    "    result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "    total_prompt_tokens += prompt_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    total_time += e2e_time\n",
    "    completion_text_list.append(completion_text)\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming completion_text_list is your new list of strings\n",
    "combined_list = []\n",
    "\n",
    "for completion_text in completion_text_list:\n",
    "    json_string = completion_text.replace('\\n', '')\n",
    "    data = json.loads(json_string)\n",
    "    if data[0][0] == \"NA\":\n",
    "        pass\n",
    "    else:\n",
    "        combined_list.append(data)\n",
    "\n",
    "# Assuming document_with_errors is your text document\n",
    "for error_list in combined_list:\n",
    "    for error_dict in error_list:\n",
    "        document_with_errors = document_with_errors.replace(error_dict[1], error_dict[2])\n",
    "document_with_corrections = document_with_errors\n",
    "# print(document_with_corrections)\n",
    "\n",
    "\n",
    "# Print totals\n",
    "# print(f\"Total Prompt Tokens: {total_prompt_tokens}\")\n",
    "# print(f\"Total Completion Tokens: {total_completion_tokens}\")\n",
    "print(f\"Total Time Taken: {total_time:.2f} seconds\")\n",
    "# print(f\"Rewritten Document: {rewritten_document}\")\n",
    "# print(f\"List of Errors to Correct: {list_of_errors_to_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the parallelization technique and make it even more concise\n",
    "\n",
    "**Time taken: 3 seconds**\n",
    "\n",
    "Because the document is not processed in a sequential order, each chunk can be analysed in parallel. This means the length of time taken to process the document is no longer proprotionate to the length of the document, and is only limited by the chunk size and the amount of compute available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "async def fetch(session, system_message, user_message):\n",
    "    url = f'{os.getenv(\"AZURE_ENDPOINT\")}/openai/deployments/gpt-4/chat/completions?api-version={os.getenv(\"API_VERSION\")}'\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": os.getenv(\"API_KEY\")\n",
    "    }  \n",
    "    data = {\n",
    "        \"model\": \"gpt-4\",  # Adjust the model as needed\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    }\n",
    "    async with session.post(url, json=data, headers=headers) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def main(system_message, user_messages):\n",
    "    api_call_batch_size = 16\n",
    "\n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        responses = []\n",
    "        for i, user_message in enumerate(user_messages):\n",
    "            # time.sleep(1) # Add a small delay to avoid hitting the rate limit\n",
    "            task = asyncio.create_task(fetch(session, system_message, user_message))\n",
    "            tasks.append(task)\n",
    "            if len(tasks) >= api_call_batch_size: \n",
    "                responses.extend(await asyncio.gather(*tasks))\n",
    "                tasks = []\n",
    "        responses.extend(await asyncio.gather(*tasks))  # Process the last batch\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 2.819835901260376 seconds\n",
      "['[[\"NA\",\"NA\",\"NA\"]]', '[[\"Spelling\",\"Initial projecctions for\",\"Initial projections for\"]]', '[[\"NA\",\"NA\",\"NA\"]]', '[[\"Plural verb agreement\",\"micro-transactions needs some\",\"micro-transactions need some\"]]', '[[\"NA\",\"NA\",\"NA\"]]', '[[\"NA\", \"NA\", \"NA\"]]', '[[\"NA\",\"NA\",\"NA\"]]', '[[\"Incorrect verb form\",\"We aim to created\",\"We aim to create\"]]', '[[\"NA\",\"NA\",\"NA\"]]']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Split the text into chunks of words\n",
    "chunks = textwrap.wrap(document_with_errors, 1000)\n",
    "# Prepend \"INPUT_DOCUMENT: \" and append \" OUTPUT_LIST:\"\n",
    "chunks = [\"INPUT_DOCUMENT: \" + chunk + \" OUTPUT_LIST:\" for chunk in chunks]\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\n",
    "###Important :\n",
    "Do not add any additional information.\n",
    "Make sure to complete all elements of the array'''\n",
    "\n",
    "The output should be a list object. Only return the list object, with no comments or additional text.\n",
    "Use this structure:\n",
    "[[\"The first item in the list is an explanation of the error. Think step-by-step about identifying potential spelling errors or grammar issues. Consider all errors together and consider the whole sentence of text before applying a rule. Explain the violation in the shortest sentence possible, ideally 3 to 7 words.\",\"The second item in this list is the specific text that includes the error, typically around 3 words either side of the error. If there is a error fill this with the errors text sub-string, consider the full sentence and context of the incorrect text before filling this in. When selecting the substring of the text, use the shortest amount of text whilst ensuring the sub string is unique in the document.\",\"The third item is the corrected text. It should be exactly the same string as the second item, but with the spelling error or grammar corrected. Think step by step about the error and then fill this with the fixed version of the text.\"]]\n",
    "\n",
    "START_EXAMPLE_1\n",
    "\n",
    "INPUT_DOCUMENT:\n",
    "Start Your Day Positively: Begin your mornings with small victories. Accomplish a tinny task, like making your bed or enjoying a cup of coffee. Set an intention for the day, whether it’s a guiding principle or a specific action you’ll take. Delayed using your phone upon waking up and replace social media scrolling with gratitude. Reflect on simple things you’re thankful for, like someone holding the door open or a warm cup of coffee from your partner.\n",
    "Prioritize Self-Care Throughout the Day: As the day progresses, continue nurturing your well-being. Savor encouraging words, read inspirational quotes, or revisit kind texts from friends. Stay active by taking short walks or practicing mindfulness. Remember that small, consistent efforts add up, contributing to your overall health and happiness. \n",
    "OUTPUT_LIST:[[\"Verb tense\",\"Delayed using your\",\"Delay using your\"],[\"Spelling\",\"a tinny task\",\"a tiny task\"]]\n",
    "\n",
    "Do not output \\n and many whitespaces.\n",
    "\n",
    "If there are no errors, return a list like this:[[\"NA\",\"NA\",\"NA\"]]. Do not generate output like this:['\\n    [[\"NA\",\"NA\",\"NA\"]]']\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_batched_call(system_message,json.dumps([chunks]), model)\n",
    "\n",
    "start = time.time()\n",
    "responses_async = await main(system_message, chunks)\n",
    "end = time.time()\n",
    "run_time = end - start\n",
    "print(f\"Total time taken: {run_time} seconds\")\n",
    "\n",
    "# responses\n",
    "completion_text_list = []\n",
    "for i in range(len(responses_async)):\n",
    "    try:\n",
    "        completion_text_list.append(responses_async[i][\"choices\"][0][\"message\"][\"content\"])\n",
    "    except:\n",
    "        pass\n",
    "# print(completion_text_list)\n",
    "# print(len(completion_text_list))\n",
    "# # Print totals\n",
    "# print(f\"Total Prompt Tokens: {prompt_tokens}\")\n",
    "# print(f\"Total Completion Tokens: {completion_tokens}\")\n",
    "# print(f\"Total Time Taken: {e2e_time:.2f} seconds\")\n",
    "# print(f\"Rewritten Document: {completion_text}\")\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming completion_text_list is your new list of strings\n",
    "combined_list = []\n",
    "\n",
    "for completion_text in completion_text_list:\n",
    "    json_string = completion_text.replace('\\n', '')\n",
    "    data = json.loads(json_string)\n",
    "    if data[0][0] == \"NA\":\n",
    "        pass\n",
    "    else:\n",
    "        combined_list.append(data)\n",
    "\n",
    "# Assuming document_with_errors is your text document\n",
    "for error_list in combined_list:\n",
    "    for error_dict in error_list:\n",
    "        document_with_errors = document_with_errors.replace(error_dict[1], error_dict[2])\n",
    "document_with_corrections = document_with_errors\n",
    "print(document_with_corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further improvements\n",
    "\n",
    "Some errors in a document may require the entire document to be analysed as a whole- for example, consistency errors in the name of a speaker (e.g. the person's name is used multiple times in the document, and spelled incorrectly in one instance). A final pass could be done over the whole document checking for errors, rather than chunking it. \n",
    "\n",
    "Similarly, there are risks of errors that overlap between different chunks. \n",
    "\n",
    "Clearly there is further work that could be done to improve this process- this is only intended as an reference of how to implement these techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
