{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Retrieval Augmented Generation\n",
    "\n",
    "This case study covers a typical Retrieval Augmented Generation (RAG) chatbot, which is providng information regarding a fictional product, the BirdBrain Automatic WiFi Pet Chicken Feeder.\n",
    "\n",
    "Two common scenarios are shown, and a number of steps implemented to show how the response can be accelerated to improve customer experience. These improvements can accelerate your application by up to 6.8x and 17x, though they are highly dependent on your specific use case, so the benefits may vary.\n",
    "\n",
    "\n",
    "There are a range of documents containing product information and troubleshooting steps that have been synthetically generated. As this case study is not about implementing RAG, these documents will be simply loaded directly into the model's prompt (no vector search will be performed) to keep the implementation as simple as possible.\n",
    "\n",
    "Whilst these concepts are relatively simple, many production applications today do not make use of them. These should be considered a starting point for ideas to further improve and build out your RAG application.\n",
    "\n",
    "If you have additional ideas, please submit a PR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('troubleshooting_information.txt', 'r') as f:\n",
    "    troubleshooting_information = f.read()\n",
    "with open('product_information.txt', 'r') as f:\n",
    "    product_information = f.read()\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import copy\n",
    "import textwrap\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def aoai_call(system_message,prompt,model):\n",
    "    client = AzureOpenAI(\n",
    "        api_version=os.getenv(\"API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"API_KEY\")\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    e2e_time = end_time - start_time\n",
    "\n",
    "    result=json.loads(completion.model_dump_json(indent=2))\n",
    "    prompt_tokens=result[\"usage\"][\"prompt_tokens\"]\n",
    "    completion_tokens=result[\"usage\"][\"completion_tokens\"]\n",
    "    completion_text=result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return result,prompt_tokens,completion_tokens,completion_text,e2e_time\n",
    "\n",
    "model=os.getenv(\"MODELGPT432k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario: Troubleshooting\n",
    "\n",
    "This scenario covers an example of a customer asking for information on how to perform basic troubleshooting of a product. This demonstrates how two techniques, _generation token compression_ and _avoid rewriting documents_ can be used to speed up the application by 6.8x.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Case\n",
    "\n",
    "**Time taken: 23 seconds**\n",
    "\n",
    "The bot has a number of context documents containing product and troubleshooting information. The user is seeking assistance to help fix the wifi. The bot is expected to provide a summary and the step by step instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens: 1085\n",
      "Completion Tokens: 331\n",
      "Time taken: 22.86 seconds\n",
      "Completion text: Here are some steps to help troubleshoot Wi-Fi issues with your BirdBrain Automatic WiFi Pet Chicken Feeder:\n",
      "\n",
      "1. **Check Wi-Fi Status:**\n",
      "    - **Device Wi-Fi Toggle:** Ensure that your device’s Wi-Fi is turned on. Visit your device settings and confirm that the Wi-Fi is activated. If not, you can switch it on from the same setting.\n",
      "    - **Phone Wi-Fi Capabilities:** Confirm that your smartphone’s Wi-Fi capabilities are working. Sometimes, modes like airplane or battery-saving can turn off Wi-Fi. If it's off, turn it back on.\n",
      "\n",
      "2. **Restart Your Router:**\n",
      "    - **Unplug and Wait:** Disconnect your router's power source. Wait for about half a minute to a minute before plugging it back again. This usually fixes most of the Wi-Fi problems.\n",
      "    - **Router Placement:** Ensure your router's placement for optimal Wi-Fi connections. A centrally located router with no major obstructions around works best.\n",
      "\n",
      "3. **Check for Outages:** Visit the website of your internet service provider (ISP) to check for any reported outages in your area. If an outage is the issue, you'll have to wait for it to be fixed.\n",
      "\n",
      "4. **Reset the WiFi Connection on BirdBrain Feeder:** If the door of the feeder has malfunctioned due to connectivity issues, it could cause Wi-Fi problems. Power off the feeder and wait for a minute before turning it back on. Then, follow the manufacturer’s instructions to reconnect the feeder to the WiFi network.\n",
      "\n",
      "If these steps don't work, consider reaching out to your manufacturer's tech support for assistance.\n"
     ]
    }
   ],
   "source": [
    "user_question=\"How can I fix the wifi\"\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\"\"\"\n",
    "prompt=f\"\"\"\n",
    "CONTEXT DOCUMENTS:\n",
    "Product information:\n",
    "{product_information}\n",
    "Troubleshooting information:\n",
    "{troubleshooting_information}\n",
    "\"\"\"\n",
    "\n",
    "prompt=prompt+f\"\"\"\n",
    "User question:\n",
    "{user_question}\n",
    "\"\"\"\n",
    "\n",
    "result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "print(f\"Prompt Tokens: {prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {completion_tokens}\")\n",
    "print(f\"Time taken: {e2e_time:.2f} seconds\")\n",
    "print(f\"Completion text: {completion_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Generation Token Compression technique\n",
    "\n",
    "**Time taken: 9.8 seconds**\n",
    "\n",
    "The less tokens the model generates, the faster the response will be. By prompting the model to be as concise as possible, the response time is significantly reduced from 23 seconds to 9.8 seconds.\n",
    "\n",
    "It is important to make sure the bot is still providing enough information to be actually helpful to the user, and a balance needs to be struck between speed and providing enough information.\n",
    "\n",
    "Few-shot prompting can be used to help guide the right level of information- this provides clear guidelines to the LLM of the level of detail expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens: 1092\n",
      "Completion Tokens: 139\n",
      "Time taken: 9.82 seconds\n",
      "Completion text: To fix the WiFi for the BirdBrain Automatic WiFi Pet Chicken Feeder, follow these steps:\n",
      "\n",
      "1. Verify your device's WiFi is on. Check in your device settings.\n",
      "2. Check your phone’s WiFi capabilities are enabled.\n",
      "3. Restart your router by unplugging it for 30 seconds to 1 minute, then plug it back in.\n",
      "4. Check its placement - it should be centrally located, elevated and away from obstructions.\n",
      "5. Visit your ISP website to check for any reported outages in your area.\n",
      "6. If the problem persists, consider resetting the WiFi connection on the feeder by powering it off and then back on, and reconnecting it to the WiFi network.\n"
     ]
    }
   ],
   "source": [
    "user_question=\"How can I fix the wifi\"\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a helpful AI assistant. Be as succint as possible.\n",
    "\"\"\"\n",
    "prompt=f\"\"\"\n",
    "CONTEXT DOCUMENTS:\n",
    "Product information:\n",
    "{product_information}\n",
    "Troubleshooting information:\n",
    "{troubleshooting_information}\n",
    "\"\"\"\n",
    "\n",
    "prompt=prompt+f\"\"\"\n",
    "User question:\n",
    "{user_question}\n",
    "\"\"\"\n",
    "\n",
    "result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "print(f\"Prompt Tokens: {prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {completion_tokens}\")\n",
    "print(f\"Time taken: {e2e_time:.2f} seconds\")\n",
    "print(f\"Completion text: {completion_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Avoid Rewriting Documents technique\n",
    "\n",
    "**Time taken: 3.4 seconds**\n",
    "\n",
    "Rather than spending significant amounts of time writing out information already contained in the knowledgebase, the LLM can use code to append the information to its response.\n",
    "\n",
    "A common approach is include a citation or link to the relevant document, or append the document/chunk in full. This approach has the advantage of only including the relevant section to the user.\n",
    "\n",
    "In this implementation, the LLM scans the context documents to find the relevant step by step information. It then returns the first three words and last three words of the relevant section. Python code is then used to extract the relevant information and append it to the short, succint response to the user.\n",
    "\n",
    "Rather than using a JSON object with key value pairs, this has been made even more efficient by using a list, saving the tokens that would need to be used to write out the keys.\n",
    "\n",
    "_Note: that this does add some complexity to the application, and this is only a rudimentary implementation of this technique. This assumes the source documents are written in a manner intended for a consumer to read._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens: 1334\n",
      "Completion Tokens: 25\n",
      "Time taken: 3.42 seconds\n",
      "Completion text: [\"YES\",\"Check Wi-Fi Status:\",\"Wi-Fi is down.\",\"Here are some troubleshooting steps for fixing your Wi-Fi.\"]\n"
     ]
    }
   ],
   "source": [
    "user_question=\"How can I fix the wifi\"\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\"\"\"\n",
    "prompt=f\"\"\"\n",
    "CONTEXT DOCUMENTS:\n",
    "Product information:\n",
    "{product_information}\n",
    "Troubleshooting information:\n",
    "{troubleshooting_information}\n",
    "\n",
    "Respond with a list object, following the structure below. If there is relevant information in the context documents, set the relevant_information flag to \"YES\", and include the first 3 words of the relevant section, and the last 3 words in the JSON below. This will be used by a python script to extract and print the results to the user. If the question does not required additional information, or it cannot be found, set the relevant_information flag to \"NO\".\n",
    "LIST_STRUCTURE:\n",
    "[\"The first item in the list is either YES or NO, indicating whether there is relevant information in the context documents\", \"The second item is the first 3 words found at the start of the relevant section.\", \"The second item is the last 3 words found at the end of the relevant section.\", \"The fourth item is a short summary of the information, responding to the user's question, ideally a single sentence.\"],\n",
    "\n",
    "USER_QUESTION:\n",
    "How can I unblock the feeder?\n",
    "LIST_OBJECT:\n",
    "[\"YES\",\"Check for Obstructions\",\"arrange for repairs.\",\"Here are instructions on unblocking the feeder!\"]\n",
    "\n",
    "USER_QUESTION:\n",
    "Hi!\n",
    "LIST_OBJECT:\n",
    "[\"NO\",\"NA\",\"NA\",\"Hi! How can I help?\"]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt=prompt+f\"\"\"\n",
    "USER_QUESTION:\n",
    "{user_question}\n",
    "LIST_OBJECT:\n",
    "\"\"\"\n",
    "\n",
    "result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "print(f\"Prompt Tokens: {prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {completion_tokens}\")\n",
    "print(f\"Time taken: {e2e_time:.2f} seconds\")\n",
    "print(f\"Completion text: {completion_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some troubleshooting steps for fixing your Wi-Fi.\n",
      "\n",
      "Relevant information:\n",
      "Device Wi-Fi Toggle:\n",
      "Ensure your device’s Wi-Fi is turned on. Sometimes, accidental taps or settings adjustments can disable Wi-Fi.\n",
      "Navigate to your device settings (usually found in the system menu or quick settings) and verify that Wi-Fi is indeed enabled.\n",
      "If it’s off, toggle it back on.\n",
      "Phone Wi-Fi Capabilities:\n",
      "Verify that your phone’s Wi-Fi capabilities are enabled. Sometimes, airplane mode or battery-saving settings can inadvertently turn off Wi-Fi.\n",
      "Double-check your phone’s Wi-Fi settings. If it’s disabled, switch it back to “on.”\n",
      "2. Restart Your Router:\n",
      "Unplug and Wait:\n",
      "Unplug your router from its power source.\n",
      "Wait for 30 seconds to 1 minute. This brief pause allows the router’s internal components to reset.\n",
      "Plug it back in and observe the blinking lights as it boots up.\n",
      "This simple step often resolves many Wi-Fi issues.\n",
      "Router Placement:\n",
      "Consider the placement of your router.\n",
      "Is it tucked away in a corner, obscured by furniture or walls? Or is it centrally located, with minimal obstructions?\n",
      "Optimal placement means central, elevated, and away from thick walls or metal objects.\n",
      "3. Check for Outages:\n",
      "ISP Outage Check:\n",
      "Visit your internet service provider’s (ISP) website. They often have an outage status page.\n",
      "Look for any reported outages in your area. If your neighbors are also experiencing Wi-Fi troubles, it might be a larger issue.\n",
      "Use your phone’s data (ironically) to access this information if your\n"
     ]
    }
   ],
   "source": [
    "combined_document=product_information+troubleshooting_information\n",
    "\n",
    "# Convert the JSON string to a Python list\n",
    "lst = json.loads(completion_text)\n",
    "\n",
    "flag = lst[0]\n",
    "\n",
    "if flag==\"YES\":\n",
    "    # The start and end substrings\n",
    "    start = lst[1]\n",
    "    end = lst[2]\n",
    "    bot_response=lst[3]\n",
    "\n",
    "    # Extract the string between the start and end substrings\n",
    "    extracted_string = combined_document.split(start)[1].split(end)[0].strip()\n",
    "\n",
    "    print(bot_response+\"\\n\\nRelevant information:\\n\"+extracted_string)\n",
    "else:\n",
    "    print(lst[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario: Frequently asked, simple questions\n",
    "\n",
    "By leveraging the Semantic Caching technique, common questions to the chatbot can be cached and reused. This can lead to speed increases of up to 17x. Here are a number of common, related questions the bot would receive:\n",
    "\n",
    "* How much does it cost? | What is the price? | How much is it?\n",
    "* What sizes of chickens can eat from it? | What breeds of chickens can use it? | What birds is it suited for?\n",
    "* How do I clean it? | What are the steps for cleaning? What do I do if it is dirty?\n",
    "\n",
    "The first time the bot enouncters one of these questions (How much does it cost?), it will take the default amount of time to answer, around 16 seconds with GPT 4 32k. It correctly responds with \"The BirdBrain Automatic WiFi Pet Chicken Feeder is priced at $89.90.\"\n",
    "\n",
    "The second time the bot sees the exact same question, it takes only 0.3-1.4s to respond, as the answer has been cached in Redis, and is now being retrieved, rather than making an entirely new call to Azure OpenAI.\n",
    "\n",
    "Let's consider a third question \"What is the price?\". As this is semantically related to \"How much does it cost?\", the bot still only takes 0.3-1.4s to respond, as it recognises that the answer should be the same for both questions.\n",
    "\n",
    "It is important to note that this approach introduces a new parameter, \"score_threshold\", which must be tuned. To low, and only questions which are extremely similar to previous questions will leverage the cache. Too high, and questions which are close but distinct in meaning will incorrectly be matched, and the wrong answer returned.\n",
    "\n",
    "This example requires setting up an Enterprise Redis Cache, in line with this tutorial:\n",
    "* https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-tutorial-semantic-cache\n",
    "\n",
    "You may wish to skip this, and simply read through the below cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import redis\n",
    "import os\n",
    "import langchain\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import RedisSemanticCache\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "AZURE_ENDPOINT=os.getenv(\"AZURE_ENDPOINT\")\n",
    "API_KEY=os.getenv(\"API_KEY\")\n",
    "API_VERSION=os.getenv(\"API_VERSION\")\n",
    "LLM_DEPLOYMENT_NAME=os.getenv(\"MODELGPT432k\")\n",
    "LLM_MODEL_NAME=os.getenv(\"MODELGPT432k\")\n",
    "EMBEDDINGS_DEPLOYMENT_NAME=os.getenv(\"EMBEDDING\")\n",
    "EMBEDDINGS_MODEL_NAME=os.getenv(\"EMBEDDING\")\n",
    "REDIS_ENDPOINT=os.getenv(\"REDIS_ENDPOINT\")\n",
    "REDIS_PASSWORD=os.getenv(\"REDIS_PASSWORD\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = API_VERSION\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_ENDPOINT\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = API_KEY\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=LLM_MODEL_NAME,\n",
    ")\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=EMBEDDINGS_MODEL_NAME,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Case\n",
    "\n",
    "**Time taken: 17 seconds**\n",
    "\n",
    "In the example of a website with a customer facing chatbot, many of the questions to the chatbot will be repeated- for example \"hello\" or \"tell me about the product\". Every time these questions are asked, the LLM is being called, using up tokens and compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question=\"How much does it cost?\"\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\"\"\"\n",
    "prompt=f\"\"\"\n",
    "CONTEXT DOCUMENTS:\n",
    "Product information:\n",
    "{product_information}\n",
    "Troubleshooting information:\n",
    "{troubleshooting_information}\n",
    "\"\"\"\n",
    "\n",
    "prompt=prompt+f\"\"\"\n",
    "User question:\n",
    "{user_question}\n",
    "\"\"\"\n",
    "\n",
    "system_message = \"You are a helpful assistant that answers questions to the best of your knowledge.\"\n",
    "\n",
    "def ask_question(question):\n",
    "    response = llm.invoke([{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": question}])\n",
    "    return response\n",
    "start_time = time.time()\n",
    "result=ask_question(prompt)\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,2)\n",
    "print(f\"The code took {elapsed_time} seconds to run.\") # note, once run, the code will be cached, so the elapsed time stored in this notebook will be less than the actual time taken to run the code for the first time!\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Semantic Caching technique\n",
    "\n",
    "**Time taken: 0.3 - 1.4 seconds**\n",
    "\n",
    "Rather than repeated calls to the API, the Semantic Caching technique is applied, which can significantly reduce costs.\n",
    "\n",
    "Care does need to be taken with this approach- refer to the technique notebook for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_url = \"rediss://:\" + REDIS_PASSWORD + \"@\"+ REDIS_ENDPOINT\n",
    "set_llm_cache(RedisSemanticCache(redis_url = redis_url, embedding=embeddings, score_threshold=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code took 0.56 seconds to run.\n",
      "content='The BirdBrain Automatic WiFi Pet Chicken Feeder is priced at $89.90.' id='run-f556823c-fa23-4a96-9282-5d45b8836abb-0'\n"
     ]
    }
   ],
   "source": [
    "user_question=\"How much does it cost?\"\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\"\"\"\n",
    "prompt=f\"\"\"\n",
    "CONTEXT DOCUMENTS:\n",
    "Product information:\n",
    "{product_information}\n",
    "Troubleshooting information:\n",
    "{troubleshooting_information}\n",
    "\"\"\"\n",
    "\n",
    "prompt=prompt+f\"\"\"\n",
    "User question:\n",
    "{user_question}\n",
    "\"\"\"\n",
    "\n",
    "system_message = \"You are a helpful assistant that answers questions to the best of your knowledge.\"\n",
    "\n",
    "def ask_question(question):\n",
    "    response = llm.invoke([{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": question}])\n",
    "    return response\n",
    "start_time = time.time()\n",
    "result=ask_question(prompt)\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,2)\n",
    "print(f\"The code took {elapsed_time} seconds to run.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code took 0.39 seconds to run.\n",
      "content='The BirdBrain Automatic WiFi Pet Chicken Feeder is priced at $89.90.' id='run-f556823c-fa23-4a96-9282-5d45b8836abb-0'\n"
     ]
    }
   ],
   "source": [
    "user_question=\"What is the price?\"\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\"\"\"\n",
    "prompt=f\"\"\"\n",
    "CONTEXT DOCUMENTS:\n",
    "Product information:\n",
    "{product_information}\n",
    "Troubleshooting information:\n",
    "{troubleshooting_information}\n",
    "\"\"\"\n",
    "\n",
    "prompt=prompt+f\"\"\"\n",
    "User question:\n",
    "{user_question}\n",
    "\"\"\"\n",
    "\n",
    "system_message = \"You are a helpful assistant that answers questions to the best of your knowledge.\"\n",
    "\n",
    "def ask_question(question):\n",
    "    response = llm.invoke([{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": question}])\n",
    "    return response\n",
    "start_time = time.time()\n",
    "result=ask_question(prompt)\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,2)\n",
    "print(f\"The code took {elapsed_time} seconds to run.\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
