{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization\n",
    "\n",
    "There are often significant opportunities to parallelize calls to Azure OpenAI for different tasks such as classification or document processing.\n",
    "\n",
    "Whilst a straightforward technique, not unique to GenAI/LLM, as seen in the document processing case study, it is important to consider how a workload could be broken up from a sequential process to a number of parallelizable tasks, which provides the opportunity to then use these approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import copy\n",
    "import textwrap\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "model = os.getenv(\"MODELGPT432k\")\n",
    "\n",
    "user_messages = [\n",
    "    \"GPT-3 (Generative Pre-trained Transformer 3) - OpenAI's powerful language model capable of writing like a human\",  \n",
    "    \"DALL-E - OpenAI's AI system that can create images from textual descriptions\",  \n",
    "    \"Scientific research has led to significant advancements in medicine and healthcare.\",  \n",
    "    \"CLIP (Contrastive Language-Image Pretraining) - OpenAI's model that understands images in the context of natural language\",  \n",
    "    \"Science has contributed to our understanding of the natural world and the universe.\",  \n",
    "    \"Codex - OpenAI's AI system that can understand and generate code, powering GitHub Copilot\",  \n",
    "    \"GPT-4 - OpenAI's rumored next iteration of their language model with anticipated improvements\",  \n",
    "    \"Azure AI - Microsoft's suite of AI services, including machine learning, cognitive services, and conversational AI\",  \n",
    "     \"The collaboration and exchange of scientific knowledge across international borders have facilitated global progress in various fields.\" ,\n",
    "     \"Scientific innovations have improved communication and connectivity through technology.\", \n",
    "    \"Microsoft Turing Models - A series of large-scale language models developed by Microsoft\",  \n",
    "    \"Microsoft Project Brainwave - Real-time AI platform for cloud and edge computing\",  \n",
    "    \"Microsoft AI for Earth - A program applying AI to environmental challenges\",  \n",
    "    \"Microsoft AI for Health - An initiative leveraging AI for health-related research\",  \n",
    "    \"Scientific innovations have improved communication and connectivity through technology.\",  \n",
    "    \"OpenAI's API - Providing access to GPT-3 and other models for various applications\",   \n",
    "]\n",
    "\n",
    "system_message = '''\n",
    "You are an helpful AI assistant that categorizes the text in one of the two categories : SCIENCE, AI. \n",
    "###Important :\n",
    "Do not add any additional information.\n",
    "Make sure to complete all elements of the array'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario: Document Classification\n",
    "\n",
    "A number of article titles are being classified as either belonging to the _Science_ or _AI_ categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: Base case - Sequential Execution\n",
    "\n",
    "**Time taken: 180 seconds**\n",
    "\n",
    "Each article is evaluated by making sequential calls, one by one, to Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\"AI\"]', '\"AI\"', '{\"category\": \"SCIENCE\"}', '[\"AI\"]', '[\"Science has contributed to our understanding of the natural world and the universe.\", \"SCIENCE\"]', '[\"Codex - OpenAI\\'s AI system that can understand and generate code, powering GitHub Copilot\", \"AI\"]', '[\"AI\"]', '[\"AI\"]', '[\"SCIENCE\"]', '{\"category\": \"SCIENCE\"}', '[\"AI\"]', '[\"AI\"]', '\"AI\"', '[\"AI\"]', '\"SCIENCE\"', '[\"AI\"]']\n",
      "Time taken for execution :  180.36897778511047\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "        api_version=os.getenv(\"API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"API_KEY\")\n",
    "    )\n",
    "\n",
    "responses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, user_message in enumerate(user_messages):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    result = completion.choices[0].message.content\n",
    "    responses.append(result)\n",
    "\n",
    "end_time = time.time()\n",
    "e2e_time = end_time - start_time\n",
    "\n",
    "print(responses)\n",
    "print(\"Time taken for execution : \", e2e_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: Batching requests\n",
    "\n",
    "**Time taken: 4.7 seconds**\n",
    "\n",
    "Rather than calling the API one by one, all of the articles are loaded as an object. This saves time establishing the network connection, loading the prompt into memory, and also allows the LLM to more efficiently generate an array of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[\"AI\", \"AI\", \"SCIENCE\", \"AI\", \"SCIENCE\", \"AI\", \"AI\", \"AI\", \"SCIENCE\", \"SCIENCE\", \"AI\", \"AI\", \"AI\", \"AI\", \"SCIENCE\", \"AI\"]',\n",
       " 4.722882986068726)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aoai_call(system_message,prompt, model):\n",
    "    client = AzureOpenAI(\n",
    "        api_version=os.getenv(\"API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"API_KEY\")\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    e2e_time = end_time - start_time\n",
    "\n",
    "    result=json.loads(completion.model_dump_json(indent=2))\n",
    "    completion_text=result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return completion_text, e2e_time\n",
    "\n",
    "aoai_call(system_message, \n",
    "          json.dumps([user_messages]), model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C: Parallelize requests using asynchronous calls\n",
    "\n",
    "**Time taken: 2.5 seconds**\n",
    "\n",
    "The API is called asynchronously, allowing an incredible speed up of the application. The repsonse time is no longer proportionate to the number of articles to classify, but now fixed to the length of a single response. There is a batch parameter, which is used to control the number of articles classified simulataneously, as the limit of the application's speed is now the amount of compute quota available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 2.5420188903808594 seconds\n",
      "['AI', 'AI', 'SCIENCE', 'AI', 'This text belongs to the category: SCIENCE.', 'AI', 'AI', 'AI', 'SCIENCE', 'SCIENCE', 'AI', 'AI', 'AI', 'AI', 'SCIENCE', 'AI']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "async def fetch(session, system_message, user_message):\n",
    "    url = f'{os.getenv(\"AZURE_ENDPOINT\")}/openai/deployments/gpt-4/chat/completions?api-version={os.getenv(\"API_VERSION\")}'\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": os.getenv(\"API_KEY\")\n",
    "    }  \n",
    "    data = {\n",
    "        \"model\": \"gpt-4\",  # Adjust the model as needed\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    }\n",
    "    async with session.post(url, json=data, headers=headers) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def main(system_message, user_messages):\n",
    "    api_call_batch_size = 16\n",
    "\n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        responses = []\n",
    "        for i, user_message in enumerate(user_messages):\n",
    "            # time.sleep(1) # Add a small delay to avoid hitting the rate limit\n",
    "            task = asyncio.create_task(fetch(session, system_message, user_message))\n",
    "            tasks.append(task)\n",
    "            if len(tasks) >= api_call_batch_size: \n",
    "                responses.extend(await asyncio.gather(*tasks))\n",
    "                tasks = []\n",
    "        responses.extend(await asyncio.gather(*tasks))  # Process the last batch\n",
    "        return responses\n",
    "\n",
    "system_message = '''\n",
    "You are an helpful AI assistant that categorizes the text in one of the two categories : SCIENCE, AI. \n",
    "###Important :\n",
    "Do not add any additional information'''\n",
    "\n",
    "start = time.time()\n",
    "responses_async = await main(system_message, user_messages)\n",
    "end = time.time()\n",
    "\n",
    "run_time = end - start\n",
    "print(f\"Total time taken: {run_time} seconds\")\n",
    "# responses\n",
    "response_content = []\n",
    "for i in range(len(responses_async)):\n",
    "    try:\n",
    "        response_content.append(responses_async[i][\"choices\"][0][\"message\"][\"content\"])\n",
    "    except:\n",
    "        pass\n",
    "print(response_content)\n",
    "print(len(response_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
